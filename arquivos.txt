import os
import dotenv
import uvicorn
from openai import OpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from langchain_qdrant import Qdrant
from pydantic import BaseModel
from fastapi import FastAPI
dotenv.load_dotenv()

class Item(BaseModel):
    query: str

model_name = "sentence-transformers/msmarco-bert-base-dot-v5"

model_kwargs = {"device": "cpu"}

encode_kwargs = {"normalize_embeddings": True}

hf = HuggingFaceEmbeddings(model_name = model_name, model_kwargs = model_kwargs, encode_kwargs = encode_kwargs) 

client_ai = OpenAI(base_url = "https://integrate.api.nvidia.com/v1",
  api_key = os.getenv("api_key_nvidea"))

client = QdrantClient("http://localhost:6333")
collection_name = "teste"

qdrant = Qdrant(
    client,
    collection_name,
    hf
  )

app = FastAPI()

@app.get("/")
async def root():
    return {"message": "Hello World"}

@app.post("/chat")
async def chat(item: Item):
  query = item.query
  
  search_result = qdrant.similarity_search(query = query, k = 10)
  
    
  list_result = []
  context = ""
  mapping = {}
  
  for i, result in enumerate(search_result):
    context += f"Contexto {i}\n{result.page_content}\n\n"
    mapping[f"Contexto {i}"] = result.metadata.get("path")
    list_result.append({"id": i, "path": result.metadata.get("path"), "content": result.page_content})
    
  rolemsg = {
    "role": "system",
    "content": f"Responda à pergunta do usuário usando documentos fornecidos no contexto. No contexto estão documentos que devem conter uma resposta.Use quantas citações e documentos forem necessários para responder à pergunta. As respostas serão em português brasil"
  }
  
  messages = [rolemsg, {"role": "user", "content": f"Documents:\n{context}\n\nQuestion: {query}"}]
  
  resposta = client_ai.chat.completions.create(
    model = "meta/llama3-70b-instruct",
    messages = messages,
    max_tokens = 1024,
    temperature = 0.3,
    stream = False
  )
  
  print(resposta)
  
  response = resposta.choices[0].message.content
  
  
  return {"response": response}
  
    

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
    

import PyPDF2
import os
# import docx
from os import listdir
from os.path import isfile, join, isdir
from qdrant_client import QdrantClient
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from qdrant_client.models import Distance, VectorParams
from langchain_qdrant import Qdrant



client = QdrantClient(url="http://localhost:6333")

#Listar diretórios e arquivos
def lista_arquivos(dir):
    arquivos_list = []
    
    for item in listdir(dir):
      if isfile(join(dir, item)):
        arquivos_list.append(join(dir, item))
      elif isdir(join(dir, item)):
         arquivos_list += lista_arquivos(join(dir, item))
    return arquivos_list

# Indexar chunks e metadata
def indexar_chunks(chunks, metadata):
  model_name = "sentence-transformers/msmarco-bert-base-dot-v5"
  model_kwargs = {"device": "cpu"}
  encode_kwargs = {"normalize_embeddings": True}
  
  hf = HuggingFaceEmbeddings(model_name = model_name, model_kwargs = model_kwargs, encode_kwargs = encode_kwargs) 
  client = QdrantClient(url="http://localhost:6333")
  collection_name = "teste"
  
  if client.collection_exists(collection_name):
    client.delete_collection(collection_name)
  
  client.create_collection(
    collection_name = collection_name,
    vectors_config = VectorParams(size = 768, distance = Distance.DOT)
  )
  
  qdrant = Qdrant(
    client,
    collection_name,
    hf
  )
  
  
  
  print(qdrant._embeddings)
  
  print("Indexando documentos...")
  
  qdrant.add_texts(chunks, metadatas = metadata)
  
  print("Documentos indexados com sucesso!")
  
#ler arquivos e dividir em chunks
def ler_arquivos(arquivos):
    all_chunks = []
    all_metadata = []
    
    for arquivo in arquivos:
        try:
            conteudo = ""
            
            if arquivo.endswith(".pdf"):
                print(f"Lendo arquivo PDF: {arquivo}")
                
                
                filename = os.path.basename(arquivo)
                title = os.path.splitext(filename)[0]
                
                read = PyPDF2.PdfReader(arquivo)
                
                
                for i, page in enumerate(read.pages):
                    page_text = page.extract_text()
                    if page_text:
                        conteudo += f"Página {i+1}: {page_text}\n\n"
                
                
                result_split = RecursiveCharacterTextSplitter(
                    chunk_size=800,
                    chunk_overlap=100,
                    separators=["\n\n", "\n", ". ", " ", ""]
                )
                
                chunks = result_split.split_text(conteudo)
                
                
                metadata = [{"path": arquivo, "title": title, "page": i // 2, "doc_type": "pdf"} 
                            for i, _ in enumerate(chunks)]
                
                all_chunks.extend(chunks)
                all_metadata.extend(metadata)
                
            
            
        except Exception as e:
            print(f"Erro ao ler arquivo {arquivo}: {e}")
    
    if all_chunks:
        indexar_chunks(all_chunks, all_metadata)
    else:
        print("Nenhum conteúdo extraído dos arquivos.")
      
  
path_list = lista_arquivos("documents")
chuck_list = ler_arquivos(path_list)

